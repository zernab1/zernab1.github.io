---
layout: post
title: Delving into Apache Pyspark
---

Alright, Day 2! I mentioned last post I wanted to learn a little about Apache Spark, and Hadoop MapReduce because that's essentially what Spark replaced in industry. But I think I'm actually going to focus mainly on Apache Pyspark for today.

I've done enough prior research to know Apache Spark is this sort of **Big Data** processing engine. The vital difference between it and MapReduce is that it processes data in real-time (*in-memory processing*), and there's multiple machines, (aka 'clusters'), working together to process this high amount of data. MapReduce is called 'Map Reduce' because it's a two parts (they call it two **batch**) processing system:
- There's the *Map* part: Processing input data and producing temporary key-value pairs, and
- The *Reduce* part: Taking those key-values and grouped them by key to reduce operations on each group

And between these two processes, Hadoop has to write to disk every time, which causes a lot of disc I/O overhead. Apache Spark fixes this issue with the in memory processing!

----

I am making notes here in tandum with learning- Spark is all about distributing the dataload to multiple clusters of machines, (to be processed in paralell, like I was talking about above). 

One of the first things I'm learning off [this](https://www.youtube.com/watch?v=QLQsW8VbTN4&t=60s&ab_channel=GregHogg) tutorial is about the `parallelize()` method that the sc SparkContext object provides. You pass it a list object and it turns it into something called a **Resilient Distributed Dataset** (or **RDD**). 

They're basically a data structure in Spark, (hence passing it a List object to create one). They have some other unique characteristics though:
* *Resilient*: The **R** In RDD. They are fault tolerant, meaning if one of the machines fails, there's something in place to recover lost data during computation.
* *Distributed*: So like I explained, there's multiple machines (nodes) that are automated to distribute out the computation work amongst themselves within a cluster. This is what allows for parallel processing in Spark.
* *Immutable*: They can't be altered once created, so you'll see this demonstrated soon but if you apply any sort of transformations on an RDD you need to create another RDD, because the original one will reman unchanged. 

Here is something else that tutorial goes through:

`nums_rdd.collect()`

You don't usually do this, it brings everything from the distributed clusters of the rdd back onto the driver machine. And you don't want that because the whole point of Spark is to run processing in paralell (distributed on clusters)!

So to take a peek at your data, use `nums_rdd.take(5)` instead, where the 5 will give you back the first 5 entries in the RDD (similar to how DataFrames work). Output:
`[0, 1, 2, 3, 4]`

Now take a look at how we create a separate RDD to hold all the numbers in our existing RDD but squared:

`squared_nums_rdd = nums_rdd.map(lambda x: x ** 2)`
`squared_nums_rdd.take(5)`

Output: `[0, 1, 4, 9, 16]`

Okay okay this is pretty self exexplanatory haha. Here's the filter method, it helps us to a sort of retrival query on RDD's if we had a RDD with key/value pairs, for example:

`pairs = squared_nums_rdd.map(lambda x: (x, len(str(x))))`
`pairs.take(25)`
`even_digit_pairs = pairs.filter(lambda x: (x[1] % 2) == 0)`
`even_digit_pairs.take(25)`

There's also ways to group info with RDD, using `groupByKey()`. Say you have this list that you turn into a RDD:

`data = [("A", 1), ("B", 2), ("A", 3), ("B", 4), ("A", 5)]`

Then you'll have an RDD of key/value pairs:

`rdd = sc.parallelize(data)`
`grouped_rdd = rdd.groupByKey()`

But there's also DataFrames in Spark! In a similar way to creating DataFrames in pandas, to do it in Pyspark you can read a csv:

`df_pyspark = spark.read.csv("tips.csv", header = True, inferSchema = True)`

Here I was looking over [this](https://medium.com/the-researchers-guide/introduction-to-pyspark-a61f7217398e) tutorial on Medium to understand how DataFrames in Spark work. In comparison to RDD, DataFrames in Spark offer more of a higher-level programming model and can overall be easier to us, (RDD's can be really powerful and neccessary to define if you need more control over the distributed data processing or partitioning aspects). 

The tutorial uses a dataset with 224 records, 7 columns. It's data collected on a waiter being tipped (how much they were tipped in US Dollar, total of the bill, sex of the person paying for the meal, etc).

This one starts off similar to the Youtube tutorial on RDD's, we create the SparkSession using `pyspark_sql.SparkSession.builder.getOrCreate()`, his syntax is a little different but doing the same thing.

Then from that SparkSession, he creates that df_pyspark DataFrame.

Calling `.show(5)` on the DataFrame will give you the first top 5 rows in the dataset. You can use pandas `.head()5` on a Pyspark DataFrame! You're just going to get the rows as a listin return.

Notice how to filter on RDD, we used lambda functions? We need to define some sort of function to return the filtered result for RDD's, but for DataFrames there's no need to define a separate function. Filtering operations are performed using DataFrame operations. Because the user doesn't need to manage the distribution of data explicitly or define a separate filtering function, DataFrame operations give us sn easier way to perform transformations on data:

To perform filtering on a DataFrame that hold chronological numbers from 0-1,000,000, and we want to return all the numbers squared (like what we did for squared_nums_rdd), we would do:
`squared_df = df.select(col("num") ** 2)`

instead. 















