---
layout: post
title: Day 2 of '#100DaysofCode'
---

Alright, Day 2! I mentioned last post I wanted to learn a little about Apache Spark, and Hadoop MapReduce because that's essentially what Spark replaced in industry. But I think I'm actually going to focus mainly on Apache Pyspark for today.

I've done enough prior research to know Apache Spark is this sort of **Big Data** processing engine. The vital difference between it and MapReduce is that it processes data in real-time (*in-memory processing*), and there's multiple machines, (aka 'clusters'), working together to process this high amount of data. MapReduce is called 'Map Reduce' because it's a two parts (they call it two **batch**) processing system:
- There's the *Map* part: Processing input data and producing temporary key-value pairs, and
- The *Reduce* part: Taking those key-values and grouped them by key to reduce operations on each group

And between these two processes, Hadoop has to write to disk every time, which causes a lot of disc I/O overhead. Apache Spark fixes this issue with the in memory processing!

----

I am making notes here in tandum with learning- Spark is all about distributing the dataload to multiple clusters of machines, (to be processed in paralell, like I was talking about above). 

One of the first things I'm learning off [this](https://www.youtube.com/watch?v=QLQsW8VbTN4&t=60s&ab_channel=GregHogg) tutorial is about the `parallelize()` method that the sc SparkContext object provides. You pass it a list object and it turns it into something called a **Resilient Distributed Dataset** (or **RDD**). 














