---
layout: post
title: Day 1 of '#100DaysofCode'
---

Alright folks! It's Day 1, what am I learning? Just in a Google Colab notebook here, doing a refresher on "4 Core ML algos":
* **Linear Regression**
* Classification
* Clustering
* Hidden Markov Models

(I'm still following along with Mr. Ruscica, and this is through his tutorial on freecodecamp btw, it's [this](https://www.youtube.com/watch?v=tPYj3fFJGjk&ab_channel=freeCodeCamp.org)) one starting out.

Before I dive into the introduction to these core learning algorithms, I want to test myself and see how much I can remember. So I'll write out my own prior knowledge on each algo prior to going through Tim's tutorial on it:

For this post today I'll probably only get to go over Linear Regression.

It is a supervised learning algo, (meaning that you feed your model *labeled* training data). For example, at UMD in Intro to ML (MSML603), our class worked on a project to predict car prices and were given a .csv file to feed into our model that contained some feature columns about cars (things like num_of_cylinders, peak_rpm, num_of_doors). 

But!! It also contained the actual car prices for each record in the .csv, and that is precisely why linear regression is supervised learning- 

In your training data, you are telling your model that, "This 4-cyl car right here? With this much mileage and this jacked up radio?...it costs $14,000." And you do that for each data record, and when it comes time for testing, your model bases predictions off that labeled training data you gave it.

<img src="{{ site.baseurl }}/images/car_price_iv.png" alt="11 car features">

Oh, also it was important for at least some of these features to hold a linear relationship between the car features and the car prices, the indep variable (IV) and dependent variable (DV), respectively. Otherwise applying Linear Regression isn't...very helpful.

But okay, how does the linear regression algo actually work? If you remember Algebra I concepts of how to plot a line, then this next part will come intuitively. Linear Regression simply takes a bunch of plotted data points, (your training data), and tries to draw the "line of best fit" through the points. 

You remember, **y= mx+b**? Yeah, it plots this best fit line by using this equation. Then uses the equation to predict future values on our test data set. **b** is where the line of best fit will slice through the y-axis (hence, sometimes calling **b** the y-intercept), **m** is the slope (rise/run).

<img src="{{ site.baseurl }}/images/line_best_fit.png" alt="line of best fit plotted">

----

Alright, continuing to go through Tim's tutorial. He goes through an example of a dataset of the people on the Titanic and some features of those people are linearly correlated to their chance of survival on the ship, (women and children are saved first in cases of emergency). So women were more likely to survive and gender played a role there, as did age because children were more likely to survive than older people.

And then he goes through a review of pandas and numpy Python libraries, just simple things like using pandas to read in a csv file into a dataframe, that the  `df.head()` function outputs the first 5 elements of that dataframe.

He also talked about the `df.describe()` function, will tell us some stats on the numerical features. Things like the avg, standard dev, count, etc. And what `df.shape()` does, which is gives us the number of rows and columns of the Dataframe (rows being number of records, columns being the number of feature columns, the dependent variables).

And then we went over some ways to generate graphs in matplotlib, like creating a histogram for the ages in the Titanic dataset. This is how you do it:

`df.age.hist(bins=20)`

<img src="{{ site.baseurl }}/images/titanic_histogram.png" alt="histogram of titanic">

We went through other visualization techniques in matplotlib, using `df.sex.value_counts().plot(kind='barh')` to create a horizontal bar chart representing a comparison between the data points tht are male vs female, and there is an obvious disparity between the two (twice as many male deaths as there are female). Something similar is going on for the first, second, and third classes of the ship.

Basically he's trying to get us to see the importance of data visualization to gauge an understanding of the data, prior to doing any model creation. Just by creating these horizontal bar graphs and histogram, we now know that more men than women died on the ship, that the demographic for the age of most deaths were in their mid 20's, and that the class that had the most deaths was the Third class.

Okay hm, this tutorial's seeming a little rudimentary to me, he's going through and explaining the importance of train/test split on our dataset. Like I explained above in my explaination of what supervised learning is, when you feed your model training data and its labeled, at some point you want to test out your model on data it hasn't seen before. That would be the *test* set, and that is one reason why its important, (at least in this algo's case), to do that train/test split.

He goes over what *epochs* are, that we give our same data to the model again but *in a different order*, allowing our model to pick up on patterns it didn't pick up on in an earlier iteration. I learned that starting with a lower amount of epochs prevents us from overfitting our data. Oh man, I gotta make sure I understand how to explain what overfitting is, don't I?

Alight, quick self explanation: Overfitting is when your model is so gung ho about what its learned from the training set that it memorizes it, and strictly begins to apply it to the test set, despite picking up on "noisy" data that isn't truly a repeatable pattern it can apply universally to other datasets. Tldr; we want to avoid this.

Anyway, there's also a term called *batches* in ML, where when we have ridiculous amounts of data and we can't feed all our data at once to be processes, we feed it in certain number of batches instead. 

The number of iterations per epoch depends on how many batches are set to process the entire dataset.
So say you have 1,000 data points and a batch size of 100. That means it would take 10 batches to complete one epoch (1,000 / 100 = 10).

Finally getting to how he creates the model, he used this Tensorflow module called `estimator.LinearClassifier`

`linear_est = tf.estimator.LinearClassifier(feature_columns=feature_columns)`

Here he's created a linear estimator, and then he calls the train() method on the estimator and passes it the training data.

`linear_est.train(train_input_fn)`
`result = linear_est.evaluate(eval_input_fn)`
`clear_output()`
`print(result['accuracy'])`

Printing out the accuracy gave us 0.76%, which is okay for the first try. We've only so far randomized the order of the data and split it in batches of 32.

Then he called the .predict() method on the linear estimator object, passing it the test data instead now. This returned a list of dicts that store a prediction (for whether the individual survived on the Titanic or not), for each of the entries in the test data. This is the histogram of the probabilities within that dict:

<img src="{{ site.baseurl }}/images/pred_titanic_probabilities.png" alt="histogram of predicted titanic probabilities">

To wrap this up, I'm not sure if this is the way I want to maintain posting on this blog. Following along video tutorials is a more time consuming way to learn. I'd much prefer working my way through documentation or written blog of a python library or tool to better understand it.

What I did like about this write up, is forcing myself to remember grad school projects and curiculum I've learned, and explain it here. I think that really helps with refreshing memory. Pretending to be a teacher is the best way to get this stuff engrained in the 'ol noggin, haha. 

I think Friday's post can be about Apache Spark and its predecessor, Hadoop's MapReduce.

Until then, take care!















