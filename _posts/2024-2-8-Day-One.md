---
layout: post
title: Day 1 of '#100DaysofCode'
---

Alright folks! It's Day 1, what am I learning? Just in a Google Colab notebook here, doing a refresher on "4 Core ML algos":
* Linear Regression
* Classification
* Clustering
* Hidden Markov Models

(I'm still following along with Mr. Ruscica, and this is through his tutorial on freecodecamp btw, its [this](https://www.youtube.com/watch?v=tPYj3fFJGjk&ab_channel=freeCodeCamp.org)) one starting out.

Before I dive into the introduction to these core learning algorithms, I want to test myself and see how much I can remember. So I'll write out my own prior knowledge on each algo prior to going through Tim's tutorial on it:

Okay, Linear Regression.

It is a supervised learning algo, (meaning that you feed your model *labeled* training data). For example, in my Intro to ML (MSML603) course at UMD, we worked on a project to predict car prices and were given a .csv file to feed into our model that contained some feature columns about cars (things like num_of_cylinders, peak_rpm, num_of_doors). 

But!! It also contained the actual car prices for each record in the .csv, and that is precisely why linear regression is supervised learning- In your training data, you are telling your model that, "This 4-cyl car right here? With this much mileage and this jacked up radio?...it costs $14,000." And you do that for each data record, and when it comes time for testing, your model bases predictions off that labeled training data you gave it.

<img src="{{ site.baseurl }}/images/car_price_iv.png" alt="11 car features";

Oh, also it was important for at least some of these features to hold a linear relationship between the car features and the car prices, the indep variable (IV) and dependent variable (DV), respectively. Otherwise applying Linear Regression isn't...very helpful.

But okay, how does the linear regression algo actually work? I remember this because it's basically Algebra I all over again. It takes a bunch of plotted data points (your training data), and tries to draw the "line of best fit" through the points. 

Y'all remember, **y= mx+b**? Yeah, it plots this best fit line by finding this equation. Then uses the equation to predict future values on our test data set. **b** is where the line of best fit will slice through the y-axis (hence, sometimes calling **b** the y-intercept), **m** is the slope (rise/run).

<img src="{{ site.baseurl }}/images/line_best_fit.png" alt="line of best fit plotted";

----

Alright, continuing to go through Tim's tutorial. He goes through an example of a dataset of the people on the Titanic and some features of those people are linearly correlated to their chance of survival on the ship, (women and children are saved first in cases of emergency). So women were more likely to survive and gender played a role there, as did age because children were more likely to survive than older people.

And then he goes through a review of pandas and numpy Python libraries, just simple things like using pandas to read in a csv file into a dataframe, that the  `df.head()` function outputs the first 5 elements of that dataframe.

He also talked about the `df.describe()` function, will tell us some stats on the numerical features. Things like the avg, standard dev, count, etc. And what `df.shape()` does, which is gives us the number of rows and columns of the Dataframe (rows being number of records, columns being the number of feature columns, the dependent variables).

And then we went over some ways to generate graphs in matplotlib, like creating a histogram for the ages in the Titanic dataset. This is how you do it:

`df.age.hist(bins=20)`

<img src="{{ site.baseurl }}/images/titanic_histogram.png" alt="histogram of titanic";

We went through other visualization techniques in matplotlib, using `df.sex.value_counts().plot(kind='barh')` to create a horizontal bar chart representing a comparison between the data points tht are male vs female, and there is an obvious disparity between the two (twice as many male deaths as there are female). Something similar is going on for the first, second, and third classes of the ship.

Basically he's trying to get us to see the importance of data visualization to gauge an understanding of the data, prior to doing any model creation. Just by creating these horizontal bar graphs and histogram, we now know that more men than women died on the ship, that the demographic for the age of most deaths were in their mid 20's, and that the class that had the most deaths was the Third class.

Okay hm, this tutorial's seeming a little rudimentary to me, he's going through and explaining the importance of train/test split on our dataset. Like I explained above in my explaination of what supervised learning is, when you feed your model training data and its labeled, at some point you want to test out your model on data it hasn't seen before. That would be the *test* set, and that is one reason why its important, (at least in this algo's case), to do that train/test split.






